<html lang=en><head><meta charset=UTF-8><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content="Systems Engineer | Rustacean | Backend Developer" name=description><meta content="Nam Dang" name=author><meta content="index, follow" name=robots><title>Nam (Viktor) Dang - Systems Engineer</title><link href=/favicon.svg rel=icon type=image/svg+xml><link as=font crossorigin href=/fonts/JetBrainsMono-Regular.woff2 rel=preload type=font/woff2><style>@font-face{font-family:JetBrains Mono;src:url(/fonts/JetBrainsMono-Regular.woff2)format("woff2");font-weight:400;font-style:normal;font-display:swap}@font-face{font-family:JetBrains Mono;src:url(/fonts/JetBrainsMono-Bold.woff2)format("woff2");font-weight:700;font-style:normal;font-display:swap}</style><link href=/style.css rel=stylesheet><body><main class=container><header class=header><div class=whoami><a href=/blog>cd ..</a></div><h1>Exploiting SIMD for HFT Latency</h1><p class=date>Date: 2026-01-20 | Read time: 1 min</header><article class=post-content><p>In High Frequency Trading, if you are processing data with scalar instructions, you are leaving money on the table. Modern CPUs are vector machines disguised as scalar ones.<h2 id=the-bottleneck>The Bottleneck</h2><p>Parsing market data feeds (e.g., NASDAQ ITCH) involves transforming millions of bytes into integers and floats. Doing this one-by-one is <strong>slow</strong>.<pre class=language-rust data-lang=rust style=color:#c0c5ce;background-color:#2b303b><code class=language-rust data-lang=rust><span style=color:#65737e>// Scalar Approach (Slow)
</span><span style=color:#b48ead>pub fn </span><span style=color:#8fa1b3>scalar_dot_product</span><span>(</span><span style=color:#bf616a>a</span><span>: &[</span><span style=color:#b48ead>f32</span><span>], </span><span style=color:#bf616a>b</span><span>: &[</span><span style=color:#b48ead>f32</span><span>]) -> </span><span style=color:#b48ead>f32 </span><span>{
</span><span>    a.</span><span style=color:#96b5b4>iter</span><span>().</span><span style=color:#96b5b4>zip</span><span>(b).</span><span style=color:#96b5b4>map</span><span>(|(</span><span style=color:#bf616a>x</span><span>, </span><span style=color:#bf616a>y</span><span>)| x * y).</span><span style=color:#96b5b4>sum</span><span>()
</span><span>}
</span></code></pre><p>The compiler attempts to auto-vectorize this, but it often fails due to strict IEEE 754 ordering rules or bounds checks.<h2 id=enter-std-simd>Enter <code>std::simd</code></h2><p>Using portable SIMD (Single Instruction, Multiple Data), we can explicitly tell the CPU to process 8 floats at once (AVX2) or 16 (AVX-512).<pre class=language-rust data-lang=rust style=color:#c0c5ce;background-color:#2b303b><code class=language-rust data-lang=rust><span>#![</span><span style=color:#bf616a>feature</span><span>(portable_simd)]
</span><span style=color:#b48ead>use </span><span>std::simd::f32x8;
</span><span>
</span><span style=color:#b48ead>pub fn </span><span style=color:#8fa1b3>vector_dot_product</span><span>(</span><span style=color:#bf616a>a</span><span>: &[</span><span style=color:#b48ead>f32</span><span>], </span><span style=color:#bf616a>b</span><span>: &[</span><span style=color:#b48ead>f32</span><span>]) -> </span><span style=color:#b48ead>f32 </span><span>{
</span><span>    </span><span style=color:#65737e>// Process 8 items at a time
</span><span>    </span><span style=color:#b48ead>let mut</span><span> chunks_a = a.</span><span style=color:#96b5b4>chunks_exact</span><span>(</span><span style=color:#d08770>8</span><span>);
</span><span>    </span><span style=color:#b48ead>let mut</span><span> chunks_b = b.</span><span style=color:#96b5b4>chunks_exact</span><span>(</span><span style=color:#d08770>8</span><span>);
</span><span>
</span><span>    </span><span style=color:#b48ead>let mut</span><span> sum = f32x8::splat(</span><span style=color:#d08770>0.0</span><span>);
</span><span>
</span><span>    </span><span style=color:#b48ead>for </span><span>(chunk_a, chunk_b) in chunks_a.</span><span style=color:#96b5b4>zip</span><span>(chunks_b) {
</span><span>        </span><span style=color:#b48ead>let</span><span> va = f32x8::from_slice(chunk_a);
</span><span>        </span><span style=color:#b48ead>let</span><span> vb = f32x8::from_slice(chunk_b);
</span><span>        sum += va * vb;
</span><span>    }
</span><span>
</span><span>    </span><span style=color:#65737e>// Reduce vector to scalar sum
</span><span>    sum.</span><span style=color:#96b5b4>reduce_sum</span><span>()
</span><span>}
</span></code></pre><h2 id=the-hardware-reality>The Hardware Reality</h2><p>Your CPU has 256-bit (YMM) or 512-bit (ZMM) registers sitting idle.<ul><li><strong>Scalar</strong>: 1 operation per cycle.<li><strong>AVX2</strong>: 8 operations per cycle.<li><strong>AVX-512</strong>: 16 operations per cycle.</ul><h2 id=real-world-results>Real World Results</h2><p>On a parsing algorithm for OB snapshot updates:<table><thead><tr><th>Method<th>Throughput<th>Latency<tbody><tr><td>Scalar<td>1.2 GB/s<td>180 ns<tr><td><strong>AVX2</strong><td><strong>5.8 GB/s</strong><td><strong>35 ns</strong></table><h2 id=conclusion>Conclusion</h2><p>Stop relying on the compiler to guess your intent. In critical hot paths, <strong>manual vectorization</strong> is the difference between getting the fill and missing the trade.</article><footer class=footer><p class=contact><code>$ contact --email nam.dt161@gmail.com --github @namdt1610</code></footer></main>